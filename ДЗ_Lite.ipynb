{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Из ноутбуков по практике \"Рекуррентные и одномерные сверточные нейронные сети\" выберите лучшую сеть, либо создайте свою.\n",
        "2. Запустите раздел \"Подготовка\"\n",
        "3. Подготовьте датасет с параметрами `VOCAB_SIZE=20'000`, `WIN_SIZE=1000`, `WIN_HOP=100`, как в ноутбуке занятия, и обучите выбранную сеть. Параметры обучения можно взять из практического занятия. Для  всех обучаемых сетей в данной работе они должны быть одни и теже.\n",
        "4. Поменяйте размер словаря tokenaizera (`VOCAB_SIZE`) на `5000`, `10000`, `40000`.  Пересоздайте датасеты, при этом оставьте `WIN_SIZE=1000`, `WIN_HOP=100`.\n",
        "Обучите выбранную нейронку на этих датасетах.  Сделайте выводы об  изменении  точности распознавания авторов текстов. Результаты сведите в таблицу\n",
        "5. Поменяйте длину отрезка текста и шаг окна разбиения текста на векторы  (`WIN_SIZE`, `WIN_HOP`) используя значения (`500`,`50`) и (`2000`,`200`). Пересоздайте датасеты, при этом оставьте `VOCAB_SIZE=20000`. Обучите выбранную нейронку на этих датасетах. Сделайте выводы об  изменении точности распознавания авторов текстов."
      ],
      "metadata": {
        "id": "ppMRe6FjzmT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек и настройка окружения\n",
        "В этом блоке производится импорт всех необходимых библиотек, которые будут использоваться в процессе выполнения задания.\n",
        "\n",
        "Работа с данными и массивами:\n",
        "\n",
        "- numpy — операции с массивами и числовыми данными.\n",
        "\n",
        "- utils — для работы с one-hot кодированием.\n",
        "\n",
        "Нейросетевые модели (Keras):\n",
        "\n",
        "- Sequential, Dense, Dropout, Embedding и другие — для построения различных типов моделей: полносвязных, рекуррентных (RNN, LSTM, GRU), сверточных (Conv1D).\n",
        "\n",
        "- Bidirectional — для двунаправленных рекуррентных слоев.\n",
        "\n",
        "Текстовая предобработка:\n",
        "\n",
        "- Tokenizer — для преобразования текстов в последовательности индексов.\n",
        "\n",
        "Визуализация:\n",
        "\n",
        "- matplotlib.pyplot — построение графиков.\n",
        "\n",
        "- ConfusionMatrixDisplay — отображение матрицы ошибок.\n",
        "\n",
        "- %matplotlib inline — отображение графиков прямо в ноутбуке.\n",
        "\n",
        "- plot_model — визуализация архитектуры сети.\n",
        "\n",
        "Прочие утилиты:\n",
        "\n",
        "- gdown — загрузка данных из Google-диска.\n",
        "\n",
        "- os, re, time — работа с файлами, временем и регулярными выражениями.\n",
        "\n",
        "- display — удобный вывод объектов в Jupyter/Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYGUAmGp-pgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Стандартная библиотека\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "import zipfile  # Для работы с архивами\n",
        "\n",
        "# Сторонние библиотеки\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import (\n",
        "    Activation,\n",
        "    BatchNormalization,\n",
        "    Bidirectional,\n",
        "    Conv1D,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    GlobalMaxPooling1D,\n",
        "    GRU,\n",
        "    LSTM,\n",
        "    MaxPooling1D,\n",
        "    SimpleRNN,\n",
        "    SpatialDropout1D,\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Подавление предупреждений\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Отображение графиков в ноутбуке\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "NuefvFnfzoSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Загрузка и распаковка датасета\n",
        "В этом блоке осуществляется автоматическая загрузка архива writers.zip из облачного хранилища Яндекс и его последующая распаковка в папку writers/. Полученные данные будут использоваться для дальнейшей подготовки выборки и обучения модели на текстах русских писателей."
      ],
      "metadata": {
        "id": "u1IGDKbY-8AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('writers.zip'):\n",
        "    # Загрузим датасет из облака\n",
        "    gdown.download('https://storage.yandexcloud.net/aiueducation/Content/base/l7/writers.zip', None, quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "inuFVTcJ0BlE",
        "outputId": "71323ca1-0274-4b41-cf2c-c36fea1111d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'writers.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Распакуем архив в папку writers\n",
        "if not os.path.exists('writers'):\n",
        "    with zipfile.ZipFile('writers.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('writers')\n",
        "\n",
        "# Выводим список файлов в папке writers\n",
        "print(\"\\n\".join(os.listdir('writers')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY6xdhUV0EMT",
        "outputId": "13780c86-7e18-4652-f5c5-0a1683e4416a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  writers.zip\n",
            "  inflating: writers/(Клиффорд_Саймак) Обучающая_5 вместе.txt  \n",
            "  inflating: writers/(Клиффорд_Саймак) Тестовая_2 вместе.txt  \n",
            "  inflating: writers/(Макс Фрай) Обучающая_5 вместе.txt  \n",
            "  inflating: writers/(Макс Фрай) Тестовая_2 вместе.txt  \n",
            "  inflating: writers/(О. Генри) Обучающая_50 вместе.txt  \n",
            "  inflating: writers/(О. Генри) Тестовая_20 вместе.txt  \n",
            "  inflating: writers/(Рэй Брэдберри) Обучающая_22 вместе.txt  \n",
            "  inflating: writers/(Рэй Брэдберри) Тестовая_8 вместе.txt  \n",
            "  inflating: writers/(Стругацкие) Обучающая_5 вместе.txt  \n",
            "  inflating: writers/(Стругацкие) Тестовая_2 вместе.txt  \n",
            "  inflating: writers/(Булгаков) Обучающая_5 вместе.txt  \n",
            "  inflating: writers/(Булгаков) Тестовая_2 вместе.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Настройка констант для структуры данных**\n",
        "\n",
        "В этом блоке задаются основные константы, необходимые для корректной загрузки и обработки данных:\n",
        "\n",
        "- FILE_DIR — имя директории, в которой находятся текстовые файлы с данными.\n",
        "\n",
        "- SIG_TRAIN и SIG_TEST — сигнатуры (подстроки) в названиях файлов, по которым будет происходить разделение на обучающую и тестовую выборки.\n"
      ],
      "metadata": {
        "id": "RFMRVX4u_Orq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка констант для загрузки данных\n",
        "FILE_DIR  = 'writers'                     # Папка с текстовыми файлами\n",
        "SIG_TRAIN = 'обучающая'                   # Признак обучающей выборки в имени файла\n",
        "SIG_TEST  = 'тестовая'                    # Признак тестовой выборки в имени файла"
      ],
      "metadata": {
        "id": "qrEewtj70P9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка и распределение текстов по классам и выборкам\n",
        "В этом блоке происходит загрузка текстов из файлов и их распределение по классам и типам выборок:\n",
        "\n",
        "- Сначала извлекаются все имена файлов из указанной директории FILE_DIR.\n",
        "\n",
        "- С помощью регулярного выражения из имени файла извлекается имя класса и тип выборки (обучающая или тестовая).\n",
        "\n",
        "- Для каждого уникального класса создаются отдельные ячейки в списках text_train и text_test.\n",
        "\n",
        "- Затем тексты из файлов добавляются к соответствующему классу и выборке (обучающей или тестовой).\n",
        "\n",
        "В результате формируются структурированные списки текстов по классам для дальнейшей обработки."
      ],
      "metadata": {
        "id": "x3Rkf4ZS_bSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовим пустые списки\n",
        "CLASS_LIST = []  # Список классов\n",
        "text_train = []  # Список для оучающей выборки\n",
        "text_test = []   # Список для тестовой выборки\n",
        "\n",
        "# Получим списка файлов в папке\n",
        "file_list = os.listdir(FILE_DIR)\n",
        "\n",
        "for file_name in file_list:\n",
        "    # Выделяем имя класса и типа выборки из имени файла\n",
        "    m = re.match('\\((.+)\\) (\\S+)_', file_name)\n",
        "    # Если выделение получилось, то файл обрабатываем\n",
        "    if m:\n",
        "\n",
        "        # Получим имя класса\n",
        "        class_name = m[1]\n",
        "\n",
        "        # Получим имя выборки\n",
        "        subset_name = m[2].lower()\n",
        "\n",
        "        # Проверим тип выборки\n",
        "        is_train = SIG_TRAIN in subset_name\n",
        "        is_test = SIG_TEST in subset_name\n",
        "\n",
        "        # Если тип выборки обучающая либо тестовая - файл обрабатываем\n",
        "        if is_train or is_test:\n",
        "\n",
        "            # Добавляем новый класс, если его еще нет в списке\n",
        "            if class_name not in CLASS_LIST:\n",
        "                print(f'Добавление класса \"{class_name}\"')\n",
        "                CLASS_LIST.append(class_name)\n",
        "\n",
        "                # Инициализируем соответствующих классу строки текста\n",
        "                text_train.append('')\n",
        "                text_test.append('')\n",
        "\n",
        "            # Найдем индекс класса для добавления содержимого файла в выборку\n",
        "            cls = CLASS_LIST.index(class_name)\n",
        "            print(f'Добавление файла \"{file_name}\" в класс \"{CLASS_LIST[cls]}\", {subset_name} выборка.')\n",
        "\n",
        "            # Откроем файл на чтение\n",
        "            with open(f'{FILE_DIR}/{file_name}', 'r') as f:\n",
        "\n",
        "                # Загрузим содержимого файла в строку\n",
        "                text = f.read()\n",
        "            # Определим выборку, куда будет добавлено содержимое\n",
        "            subset = text_train if is_train else text_test\n",
        "\n",
        "            # Добавим текста к соответствующей выборке класса. Концы строк заменяются на пробел\n",
        "            subset[cls] += ' ' + text.replace('\\n', ' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icC2zaR10Spi",
        "outputId": "6692ccd3-50bf-46b1-86c6-3ebccde9130a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Добавление класса \"Макс Фрай\"\n",
            "Добавление файла \"(Макс Фрай) Обучающая_5 вместе.txt\" в класс \"Макс Фрай\", обучающая выборка.\n",
            "Добавление класса \"Рэй Брэдберри\"\n",
            "Добавление файла \"(Рэй Брэдберри) Тестовая_8 вместе.txt\" в класс \"Рэй Брэдберри\", тестовая выборка.\n",
            "Добавление класса \"Клиффорд_Саймак\"\n",
            "Добавление файла \"(Клиффорд_Саймак) Тестовая_2 вместе.txt\" в класс \"Клиффорд_Саймак\", тестовая выборка.\n",
            "Добавление класса \"О. Генри\"\n",
            "Добавление файла \"(О. Генри) Тестовая_20 вместе.txt\" в класс \"О. Генри\", тестовая выборка.\n",
            "Добавление файла \"(О. Генри) Обучающая_50 вместе.txt\" в класс \"О. Генри\", обучающая выборка.\n",
            "Добавление класса \"Стругацкие\"\n",
            "Добавление файла \"(Стругацкие) Тестовая_2 вместе.txt\" в класс \"Стругацкие\", тестовая выборка.\n",
            "Добавление класса \"Булгаков\"\n",
            "Добавление файла \"(Булгаков) Обучающая_5 вместе.txt\" в класс \"Булгаков\", обучающая выборка.\n",
            "Добавление файла \"(Булгаков) Тестовая_2 вместе.txt\" в класс \"Булгаков\", тестовая выборка.\n",
            "Добавление файла \"(Клиффорд_Саймак) Обучающая_5 вместе.txt\" в класс \"Клиффорд_Саймак\", обучающая выборка.\n",
            "Добавление файла \"(Рэй Брэдберри) Обучающая_22 вместе.txt\" в класс \"Рэй Брэдберри\", обучающая выборка.\n",
            "Добавление файла \"(Стругацкие) Обучающая_5 вместе.txt\" в класс \"Стругацкие\", обучающая выборка.\n",
            "Добавление файла \"(Макс Фрай) Тестовая_2 вместе.txt\" в класс \"Макс Фрай\", тестовая выборка.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Определение количества классов**\n",
        "\n",
        "После загрузки и распределения текстов определяется общее количество уникальных классов (авторов). Это необходимо для последующих шагов, включая one-hot кодирование меток и настройку выходного слоя нейросети.\n",
        "\n",
        "- CLASS_COUNT — число уникальных классов, равное длине списка CLASS_LIST."
      ],
      "metadata": {
        "id": "aR7uSKEp_uoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определим количество классов\n",
        "CLASS_COUNT = len(CLASS_LIST)"
      ],
      "metadata": {
        "id": "hy_bWKK00Vaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод списка классов**\n",
        "\n",
        "Этот блок выводит перечень всех уникальных классов (авторов), чьи тексты были загружены из датасета."
      ],
      "metadata": {
        "id": "N_2fVJzN_8No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Выведем прочитанные классы текстов\n",
        "print(CLASS_LIST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSU_GesQ0X_r",
        "outputId": "a99f2426-fe16-48e4-d819-9626156a6607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Макс Фрай', 'Рэй Брэдберри', 'Клиффорд_Саймак', 'О. Генри', 'Стругацкие', 'Булгаков']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Подсчет количества обучающих текстов**\n",
        "\n",
        "Блок выводит количество обучающих текстов, хранящихся в списке text_train.\n",
        "Каждый элемент этого списка соответствует одному классу (автору), и содержит объединённый текст всех его обучающих файлов."
      ],
      "metadata": {
        "id": "r5z7NkV4ALF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Посчитаем количество текстов в обучающей выборке\n",
        "print(len(text_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrtrxcG60bdM",
        "outputId": "48a142c3-4ad7-4992-d36d-cbdf7daa590d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Просмотр фрагментов текстов по классам**\n",
        "\n",
        "Цикл перебирает все классы (авторов) в датасете и выводит:\n",
        "\n",
        "- Название каждого класса.\n",
        "\n",
        "- Первые 200 символов из обучающего текста (text_train) этого класса.\n",
        "\n",
        "- Первые 200 символов из тестового текста (text_test) этого класса.\n",
        "\n",
        "Это позволяет убедиться, что данные загружены корректно и дают общее представление о содержимом выборок.\n"
      ],
      "metadata": {
        "id": "cKUP-CzQAYqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим загрузки: выведем начальные отрывки из каждого класса\n",
        "\n",
        "for cls in range(CLASS_COUNT):                   # Запустим цикл по числу классов\n",
        "    print(f'Класс: {CLASS_LIST[cls]}')           # Выведем имя класса\n",
        "    print(f'  train: {text_train[cls][:200]}')   # Выведем фрагмент обучающей выборки\n",
        "    print(f'  test : {text_test[cls][:200]}')    # Выведем фрагмент тестовой выборки\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FaTm0Ef0d3C",
        "outputId": "0534bfd3-4df7-4ef1-b934-23292d7e5a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Класс: Макс Фрай\n",
            "  train:  ﻿Власть несбывшегося   – С тех пор как меня угораздило побывать в этой грешной Черхавле, мне ежедневно снится какая-то дичь! – сердито сказал я Джуффину. – Сглазили они меня, что ли? А собственно, по\n",
            "  test :  ﻿Слишком много кошмаров    Когда балансируешь над пропастью на узкой, скользкой от крови доске, ответ на закономерный вопрос: «Как меня сюда занесло?» – вряд ли принесёт практическую пользу. Зато пои\n",
            "\n",
            "Класс: Рэй Брэдберри\n",
            "  train:  ﻿451° по Фаренгейту   ДОНУ КОНГДОНУ С БЛАГОДАРНОСТЬЮ   Если тебе дадут линованную бумагу, пиши поперёк.  Хуан Рамон Хименес   Часть 1  ОЧАГ И САЛАМАНДРА   Жечь было наслаждением. Какое-то особое насл\n",
            "  test :  ﻿Марсианские хроники   МОЕЙ ЖЕНЕ МАРГАРЕТ С ИСКРЕННЕЙ ЛЮБОВЬЮ   «Великое дело – способность удивляться, – сказал философ. – Космические полеты снова сделали всех нас детьми».   Январь 1999  Ракетное \n",
            "\n",
            "Класс: Клиффорд_Саймак\n",
            "  train:  ﻿Всё живое...     Когда я выехал из нашего городишка и повернул на шоссе, позади оказался грузовик. Этакая тяжелая громадина с прицепом, и неслась она во весь дух. Шоссе здесь срезает угол городка, и\n",
            "  test :  ﻿Зачарованное паломничество    1  Гоблин со стропил следил за прячущимся монахом, который шпионил за ученым. Гоблин ненавидел монаха и имел для этого все основания. Монах никого не ненавидел и не люб\n",
            "\n",
            "Класс: О. Генри\n",
            "  train:  «Лиса-на-рассвете»   Коралио нежился в полуденном зное, как томная красавица в сурово хранимом гареме. Город лежал у самого моря на полоске наносной земли. Он казался брильянтиком, вкрапленным в ярко\n",
            "  test :  ﻿Багдадская птица   Без всякого сомнения, дух и гений калифа Гаруна аль-Рашида осенил маркграфа Августа-Михаила фон Паульсена Квигга.  Ресторан Квигга находится на Четвертой авеню — на улице, которую\n",
            "\n",
            "Класс: Стругацкие\n",
            "  train:  Парень из преисподней     1     Ну и деревня! Сроду я таких деревень не видел и не знал даже, что такие деревни бывают. Дома круглые, бурые, без окон, торчат на сваях, как сторожевые вышки, а под ним\n",
            "  test :  ﻿ОТЕЛЬ «У ПОГИБШЕГО АЛЬПИНИСТА»    ГЛАВА 1     Я остановил машину, вылез и снял черные очки. Все было так, как рассказывал Згут. Отель был двухэтажный, желтый с зеленым, над крыльцом красовалась трау\n",
            "\n",
            "Класс: Булгаков\n",
            "  train:  ﻿Белая гвардия   Посвящается[1]  Любови Евгеньевне Белозерской[2]  Пошел мелкий снег и вдруг повалил хло-  пьями. Ветер завыл; сделалась метель.  В одно мгновение темное небо смешалось с  снежным мор\n",
            "  test :  ﻿Дон Кихот ДЕЙСТВУЮЩИЕ ЛИЦА Алонсо Кихано, он же Дон Кихот Ламанчский.  Антония – его племянница.  Ключница Дон Кихота.  Санчо Панса – оруженосец Дон Кихота.  Перо Перес – деревенский священник, лице\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Контекстный менеджер для измерения времени выполнения**\n",
        "\n",
        "Создается класс timex, реализующий контекстный менеджер для удобного измерения времени выполнения блока кода.\n",
        "\n",
        "- При входе (__enter__) фиксируется текущее время.\n",
        "\n",
        "- При выходе (__exit__) выводится время, затраченное на выполнение блока.\n"
      ],
      "metadata": {
        "id": "ebpp6aMUAoOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Контекстный менеджер для измерения времени операций\n",
        "# Операция обертывается менеджером с помощью оператора with\n",
        "\n",
        "class timex:\n",
        "    def __enter__(self):\n",
        "        # Фиксация времени старта процесса\n",
        "        self.t = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        # Вывод времени работы\n",
        "        print('Время обработки: {:.2f} с'.format(time.time() - self.t))"
      ],
      "metadata": {
        "id": "HRgjpsx00e16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Настройка параметров эксперимента\n",
        "Задаются параметры для экспериментов с различными размерами словаря токенизатора и параметрами окна сегментации текста:\n",
        "\n",
        "- VOCAB_SIZES — список значений размера словаря для токенизатора (от 5000 до 40000).\n",
        "\n",
        "- WIN_SIZES — список длин окон (фрагментов текста) для разбиения.\n",
        "\n",
        "- WIN_HOPS — соответствующие шаги (смещения) окна при сегментации текста.\n",
        "\n",
        "Также фиксируются параметры обучения нейронных сетей:\n",
        "\n",
        "- число эпох (EPOCHS) — 5,\n",
        "\n",
        "- размер батча (BATCH_SIZE) — 128."
      ],
      "metadata": {
        "id": "PJW9oAl-A2SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZES = [5000, 10000, 20000, 40000]  # изменение размера словаря токенизатора\n",
        "WIN_SIZES = [1000, 500, 2000]   # Размеры окна сегментации текста\n",
        "WIN_HOPS  = [100, 50, 200]  # Шаг окна сегментации текста\n",
        "\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "UWVbqChC5-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка датасета с параметрами\n",
        "Функция prepare_dataset отвечает за подготовку обучающих и тестовых данных для нейросети с учетом заданных параметров: размера словаря (vocab_size), длины окна сегментации текста (win_size) и шага окна (win_hop).\n",
        "\n",
        "Внутри функции:\n",
        "\n",
        "- создаётся токенизатор с ограничением словаря по размеру и обработкой неизвестных слов;\n",
        "\n",
        "- выполняется разбиение каждого текста из обучающей и тестовой выборки на последовательности фиксированной длины с заданным шагом;\n",
        "\n",
        "- применяется паддинг последовательностей до фиксированной длины окна;\n",
        "\n",
        "- метки классов преобразуются в формат one-hot encoding;\n",
        "\n",
        "- возвращаются подготовленные массивы признаков и меток для обучения и тестирования, а также сам токенизатор.\n",
        "\n",
        "Этот блок позволяет гибко формировать датасеты под разные параметры эксперимента с размерами словаря и окон сегментации."
      ],
      "metadata": {
        "id": "ZPCU3pyYBCEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Функция подготовки датасета\n",
        "# Подготовка датасета с разными VOCAB_SIZE, WIN_SIZE, WIN_HOP.\n",
        "def prepare_dataset(vocab_size, win_size, win_hop):\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(text_train + text_test)\n",
        "\n",
        "    # Разбивка текста на отрезки фиксированной длины с заданным шагом.\n",
        "    # Реализация параметров WIN_SIZE, WIN_HOP.\n",
        "    def split_texts(texts):\n",
        "        sequences = []\n",
        "        labels = []\n",
        "        for idx, text in enumerate(texts):\n",
        "            seq = tokenizer.texts_to_sequences([text])[0]\n",
        "            for i in range(0, len(seq) - win_size, win_hop):\n",
        "                chunk = seq[i:i+win_size]\n",
        "                sequences.append(chunk)\n",
        "                labels.append(idx)\n",
        "        return sequences, labels\n",
        "\n",
        "    # Подготовка входных и выходных данных\n",
        "    # Паддинг последовательностей и one-hot-кодировка меток.\n",
        "    X_train, y_train = split_texts(text_train)\n",
        "    X_test, y_test = split_texts(text_test)\n",
        "\n",
        "    X_train = pad_sequences(X_train, maxlen=win_size)\n",
        "    X_test = pad_sequences(X_test, maxlen=win_size)\n",
        "\n",
        "    y_train = utils.to_categorical(y_train, CLASS_COUNT)\n",
        "    y_test = utils.to_categorical(y_test, CLASS_COUNT)\n",
        "\n",
        "    return np.array(X_train), np.array(X_test), y_train, y_test, tokenizer"
      ],
      "metadata": {
        "id": "Xt2Oh4Pb6BGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Функция построения рекуррентной нейронной сети\n",
        "Функция build_model создаёт модель последовательной нейронной сети с использованием слоев:\n",
        "\n",
        "- Embedding — преобразует входные индексы слов в векторное представление размерности 128, с длиной входной последовательности win_size.\n",
        "\n",
        "- SpatialDropout1D — применяется дропаут для регуляризации, предотвращая переобучение.\n",
        "\n",
        "- Bidirectional LSTM — двухнаправленная LSTM с 64 нейронами, которая учитывает контекст текста в обеих направлениях; return_sequences=False означает, что выводится только последний временной шаг.\n",
        "\n",
        "- Dropout — дополнительный слой дропаут для регуляризации.\n",
        "\n",
        "- Dense с функцией активации softmax — классификатор на количество классов CLASS_COUNT.\n",
        "\n",
        "Модель компилируется с функцией потерь categorical_crossentropy, оптимизатором adam и метрикой точности accuracy."
      ],
      "metadata": {
        "id": "xGIVSFlKBXiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, win_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 128, input_length=win_size))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "rH4fgRBiDAik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Изменение размера словаря (VOCAB_SIZE) и обучение модели\n",
        "В этом блоке происходит экспериментальное обучение модели при разных размерах словаря токенизатора (VOCAB_SIZE):\n",
        "\n",
        "- Для каждого значения из списка VOCAB_SIZES (5000, 10000, 20000, 40000):\n",
        "\n",
        "- Подготавливается датасет с фиксированной длиной окна WIN_SIZE=1000 и шагом окна WIN_HOP=100 с помощью функции prepare_dataset.\n",
        "\n",
        "- Строится и компилируется модель build_model с текущим размером словаря.\n",
        "\n",
        "- Модель обучается в течение EPOCHS эпох, с размером батча BATCH_SIZE. Обучение проводится без подробного вывода (verbose=0).\n",
        "\n",
        "- По окончании обучения оценивается точность модели на тестовом наборе. Результаты (VOCAB_SIZE и точность) сохраняются.\n",
        "\n",
        "- Время каждого цикла измеряется с помощью контекстного менеджера timex.\n",
        "\n",
        "В конце выводится итоговая таблица с точностью классификации для каждого размера словаря.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dh3DYGVWBte7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_vocab = []\n",
        "\n",
        "# Задание: VOCAB_SIZE изменение\n",
        "for vocab_size in VOCAB_SIZES:\n",
        "    print(f'\\n\\n=== VOCAB_SIZE = {vocab_size} ===')\n",
        "    with timex():\n",
        "        X_train, X_test, y_train, y_test, _ = prepare_dataset(vocab_size, 1000, 100)\n",
        "        model = build_model(vocab_size, 1000)\n",
        "        history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                            validation_data=(X_test, y_test), verbose=0)\n",
        "        acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "        results_vocab.append((vocab_size, acc))\n",
        "        print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "# Таблица результатов по VOCAB_SIZE\n",
        "print(\"\\nТочность для разных размеров словаря:\")\n",
        "for vocab_size, acc in results_vocab:\n",
        "    print(f'VOCAB_SIZE={vocab_size:<6} --> Accuracy={acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVo8aCqLGOFV",
        "outputId": "891b4ad2-4340-47b8-8128-fc78612da7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== VOCAB_SIZE = 5000 ===\n",
            "Accuracy: 0.6775\n",
            "Время обработки: 118.22 с\n",
            "\n",
            "\n",
            "=== VOCAB_SIZE = 10000 ===\n",
            "Accuracy: 0.6081\n",
            "Время обработки: 118.03 с\n",
            "\n",
            "\n",
            "=== VOCAB_SIZE = 20000 ===\n",
            "Accuracy: 0.6326\n",
            "Время обработки: 117.60 с\n",
            "\n",
            "\n",
            "=== VOCAB_SIZE = 40000 ===\n",
            "Accuracy: 0.7150\n",
            "Время обработки: 118.81 с\n",
            "\n",
            "Точность для разных размеров словаря:\n",
            "VOCAB_SIZE=5000   --> Accuracy=0.6775\n",
            "VOCAB_SIZE=10000  --> Accuracy=0.6081\n",
            "VOCAB_SIZE=20000  --> Accuracy=0.6326\n",
            "VOCAB_SIZE=40000  --> Accuracy=0.7150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Изменение параметров окна сегментации текста (WIN_SIZE, WIN_HOP)**\n",
        "\n",
        "В этом блоке исследуется влияние длины окна (WIN_SIZE) и шага окна (WIN_HOP) на качество классификации:\n",
        "\n",
        "- Для каждой пары параметров (WIN_SIZE, WIN_HOP) из списка [(500, 50), (2000, 200)]:\n",
        "\n",
        "- Готовится датасет с фиксированным размером словаря VOCAB_SIZE=20000, а длина и шаг окна задаются текущими значениями.\n",
        "\n",
        "- Строится и компилируется модель с текущей длиной окна.\n",
        "\n",
        "- Модель обучается в течение заданного количества эпох (EPOCHS) и размера батча (BATCH_SIZE).\n",
        "\n",
        "- После обучения вычисляется точность модели на тестовом наборе, и результаты сохраняются.\n",
        "\n",
        "- Время подготовки и обучения фиксируется с помощью контекстного менеджера timex.\n",
        "\n",
        "Итогом работы блока является таблица с точностью классификации для каждого варианта параметров окна."
      ],
      "metadata": {
        "id": "7106jBciB-U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_win = []\n",
        "\n",
        "for win_size, win_hop in [(500,50), (2000,200)]:\n",
        "    print(f'\\n\\n=== WIN_SIZE = {win_size}, WIN_HOP = {win_hop} ===')\n",
        "    with timex():\n",
        "        X_train, X_test, y_train, y_test, _ = prepare_dataset(20000, win_size, win_hop)\n",
        "        model = build_model(20000, win_size)\n",
        "        history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                            validation_data=(X_test, y_test), verbose=0)\n",
        "        acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "        results_win.append((win_size, win_hop, acc))\n",
        "        print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "# Таблица результатов по WIN_SIZE/WIN_HOP\n",
        "print(\"\\nТочность для разных параметров окна:\")\n",
        "for win_size, win_hop, acc in results_win:\n",
        "    print(f'WIN_SIZE={win_size:<5}, WIN_HOP={win_hop:<4} --> Accuracy={acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCSvByMlGeDp",
        "outputId": "93d56ab9-5bc1-450c-b7d2-284867cb3268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== WIN_SIZE = 500, WIN_HOP = 50 ===\n",
            "Accuracy: 0.7291\n",
            "Время обработки: 119.90 с\n",
            "\n",
            "\n",
            "=== WIN_SIZE = 2000, WIN_HOP = 200 ===\n",
            "Accuracy: 0.4405\n",
            "Время обработки: 118.99 с\n",
            "\n",
            "Точность для разных параметров окна:\n",
            "WIN_SIZE=500  , WIN_HOP=50   --> Accuracy=0.7291\n",
            "WIN_SIZE=2000 , WIN_HOP=200  --> Accuracy=0.4405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод по заданию\n",
        "Отсюда по заданию видно, что лучший результат даёт небольшое окно (WIN_SIZE=500) с малым шагом (WIN_HOP=50), то есть это даёт больше обучающих примеров, а также модель чаще видит начало и конец текста, и повышается устойчивость и обобщаемость.\n",
        "\n",
        "Длинные окна (WIN_SIZE=2000) ухудшают результат, так как такие окна покрывают слишком большую часть текста, их меньше по количеству, а также труднее выделить локальные стилистические признаки автора. Так же возможна перегрузка модели лишней информацией. Отсюда чем больше разнообразие обучающих примеров (много коротких окон), тем лучше модель учится различать стили разных авторов.\n",
        "\n",
        "В ходе экспериментов с различными размерами словаря токенизатора (VOCAB_SIZE) было выявлено, что наибольшая точность классификации — 71.5% — достигается при максимальном размере словаря в 40,000 слов. Это свидетельствует о том, что увеличение размера словаря позволяет модели лучше охватывать разнообразие лексики и тем самым эффективнее распознавать авторов текстов. При этом минимальный размер словаря в 5,000 слов обеспечил точность 67.75%, что говорит о том, что даже небольшой словарь может сохранять важную информацию для классификации, однако охват слов становится ограничен. Интересный факт заключается в том, что при размере словаря 10,000 точность заметно снизилась до 60.81%, что может быть связано с особенностями распределения слов и их частоты в корпусе. При размере 20,000 слов точность немного улучшилась до 63.26%, но все еще уступала результату при 40,000."
      ],
      "metadata": {
        "id": "8y8JXbqyCPro"
      }
    }
  ]
}